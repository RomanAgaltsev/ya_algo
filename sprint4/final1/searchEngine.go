/*
Яндекс Практикум
Алгоритмы и структуры данных
58 когорта
Агальцев Роман

Спринт 4
Задача - A. Поисковая система

Отчеты:
- Ревью 1 - https://contest.yandex.ru/contest/24414/run-report/113388692/
- Ревью 2 - https://contest.yandex.ru/contest/24414/run-report/113590334/

-- ПРИНЦИП РАБОТЫ --
Как и в предыдущем спринте, прошел несколько вариантов решений.

Вариант 1 - https://contest.yandex.ru/contest/24414/run-report/113344736/
В качестве поискового индекса использовалась мапа map[int]int, в которой хранились количества вхождений (значения) в разрезе слов (ключи).
Такой поисковый индекс хранился для каждого документа отдельно. В качестве ключа рассчитывался хэш слова.
Документы считывались, строился поисковый индекс.
Далее, запросы считывались, получались уникальные слова, рассчитывались их хэши, рассчитывались релевантности документов.
Решение проходило первые 15 тестов, а следующие не проходили по time-limit-exceeded - 6 секунд - 6.06s/ 23.04Mb.

После некоторых размышлений понял, что много времени тратится на обход поисковых индексов документов, а слова в них может и не быть.
То есть, надо было отсекать документы, в которых слова из запроса не было. Надо было переделывать поисковый индекс.

Вариант 2 - https://contest.yandex.ru/contest/24414/run-report/113346649/
Переделал поисковый индекс на более сложную структуру - мапа map[int]map[int]int.
В мапе в качестве ключа использовался хэш слова, а в качестве значения - мапа[номер документа]количество вхождений.
Такая структура поискового индекса позволила для расчета релевантностей не обходить документы, в которых слова точно нет.
Решение проходило первые 15 тестов, а следующие не проходили по wrong-answer - 2.796s/90.11Mb

Опять поразмыслив предположил, что проблема в коллизиях при формировании хэшей.
По времени и памяти запас был, решил убрать хэши и работать со строками.

Вариант 3 - https://contest.yandex.ru/contest/24414/run-report/113348236/
Переделал поисковый индекс на map[string]map[int]int - заменил ключ на строку. Также убрал использование хэшей.
Решение стало проходить все тесты - 2.771s/103.39Mb.

Решил немного структурировать решение и реализовал для поисковой системы структуру с методами для построения индекса и обработки запросов.

Вариант 4 (итоговый) - https://contest.yandex.ru/contest/24414/run-report/113388692/ - 3.639s/102.23Mb
Для решения используется структура SearchEngine с двумя методами - updateIndex и processQuery.

В структуре хранится:
- Поисковый индекс map[string]map[int]int
- Ограничитель на количество выводимых номеров документов

Метод updateIndex - по переданным номеру документа и строке-документу обновляет поисковый индекс:
- Строку-документ разбиваем на слова по пробелам
- Каждое слово складываем в поисковый индекс. Ключ - само слово, значение - мапа[номер документа]количество вхождений

Метод processQuery - обрабатывает полученную строку-запрос и возвращает релевантные документы:
- Из строки-запроса получаем его уникальные слова
- По каждому уникальному слову запроса рассчитываем суммы вхождений в разрезе документов - релевантности
- Получившиеся релевантности сортируем при помощи сортировки стандартной библиотеки и собственного компаратора
- Формируем результат обработки с учетом имеющегося ограничителя на вывод

Последовательность следующая:
- Создаем структуру - поисковую систему - сразу задаем ограничитель вывода
- Считываем строки-документы с ввода и по каждой вызываем updateIndex структуры для обновления поискового индекса
- Считываем строки-запросы с ввода и каждую отдельно обрабатываем при помощи processQuery структуры
- Результат обработки каждого запроса складываем в слайс, содержимое которого выводим по окончанию обработки всех запросов

Вариант 4 вернулся с ревью с замечанием - сократить сложность получения топа релевантностей с O(nlogn) до O(n).

Вариант 5 - https://contest.yandex.ru/contest/24414/run-report/113590334/ - 1.832s/108.54Mb
Простой поиск со сложностью O(n) - это обычный обход массива.
Так как в качестве решения надо вывести только несколько релевантностей (топ),
то можно для каждой искомой релевантности просто выполнить обход слайса с релевантностями.

Это даст ту же сложность O(n), так как у нас есть ограничитель-константа (5 по задаче).
Сложность O(5*n) - линейная. При отбрасывании константы даст ту же O(n).

На основании этого скорректировал решение:
- Компаратор вынес в отдельную функцию;
- Удалил сортировку слайса;
- Добавил обычный обход слайса релевантностей в количестве итераций, равном ограничителю;
- Добавил вложенный цикл для обхода слайса и перемещения максимальных релевантностей (топа) в начало слайса.

Ресурсы, требуемые для выполнения решения сократились до 1.832s/108.54Mb.
Таким образом, время работы решения сократилось в 2 раза!

-- ДОКАЗАТЕЛЬСТВО КОРРЕКТНОСТИ --
Из описания задачи можно сформулировать подзадачи:
1. Реализовать поисковый индекс по документам, полученным с ввода - поисковый индекс реализован;
2. Реализовать обработку запросов, полученных с ввода, с выводом 5 самых релевантных документов с сортировкой по убыванию релевантности - реализовано.

Решение проходит тесты Контеста в отведенные время и память - вердикт ОК.

-- ВРЕМЕННАЯ СЛОЖНОСТЬ --
Примем следующие обозначения:
N - количество слов в индексируемых документах
M - количество слов в запросах
K - количество уникальных слов в запросах
L - количество документов

Построение поискового индекса - O(N)
Даже если слова повторяются, нам надо всех их посчитать, то есть, обработать каждое слово документа.

Обработка входщих запросов, получение уникальных слов - O(M)
Опять же, обработать надо каждое слово запроса.

Подсчет релевантностей по уникальным словам запросов - O(K * L)
Можно предположить, что поиск одного слова в индексе - это O(1).
В худшем случае, каждое уникальное слово запроса может присутствовать в каждом документе поиского индекса.
Поэтому, количество уникальных слов умножаем на количество документов.

В итоге, получается - O(N + M + K*L)

-- ПРОСТРАНСТВЕННАЯ СЛОЖНОСТЬ --
Примем следующие обозначения:
N - количество слов в индексируемых документах
M - количество слов в запросах
K - количество уникальных слов в запросах
L - количество документов
P - количество запросов

В решении используется память для:
- Поисковый индекс - O(N * L) - количество слов умноженное на количество документов
- Уникальные слова запросов - O(K)
- Результаты обработки запросов - O(5 * P)

В итоге, получается - O(N*L + K + 5*P)

*/

package main

import (
	"bufio"
	"cmp"
	"os"
	"strconv"
	"strings"
)

// SearchEngine - структура поисковой системы
type SearchEngine struct {
	index map[string]map[int]int // Поисковый индекс - мапа[слово]мапа[номер документа]количество вхождений
	limit int // Ограничение количества выводимых в результате документов
}

// updateIndex - обновляет поисковый индекс по переданным номеру документа и документу (строке)
func (s *SearchEngine) updateIndex(docNumber int, doc string) {
	// Полученный документ-строку разбиваем по пробелам на слова и обходим в цикле
	for _, word := range strings.Split(doc, " ") {
		// Проверяем, присутствует ли слово в индексе
		if wordsCount, ok := s.index[word]; ok {
			// Слово в индексе присутствует, увеличиваем счетчик по номеру документа
			// Номер документа сразу увеличиваем на 1, чтобы уйти от индексов
			wordsCount[docNumber+1]++
		} else {
			// Слово в индексе отсутствует, инициируем мапу[номер документа]количество вхождений
			// И сразу записываем 1
			s.index[word] = map[int]int{docNumber + 1: 1}
		}
	}
}

// processQuery - обрабатывает строку запроса и возвращает релевантные документы
func (s *SearchEngine) processQuery(query string) []int {
	// Инициируем мапу для хранения количеств слов в разрезе номеров документов
	wordsCount := make(map[int]int)
	// Получаем уникальные слова запроса
	words := getUniqueWords(query)
	// Обходим уникальные слова запроса и считаем количество вхождений в документы-строки
	for _, word := range words {
		// Проверяем, присутствует ли слово запроса в поисковом индексе
		// Если слова запроса вообще нет в индексе, то и обходить документы для подсчета нет смысла - пустая трата
		if docs, ok := s.index[word]; ok {
			// Слово запроса в индексе есть, считаем - обходим мапу[номер документа]количество вхождений
			for doc, count := range docs {
				// Прибавляем количество вхождений текущего слова запроса к общей сумме вхождений всех слов запроса в документ
				wordsCount[doc] += count
			}
		}
	}
	// Инициируем слайс массивов по два элемента [номер документа, релевантность] для сортировки
	wordsCountSorted := make([][2]int, 0)
	// Обходим мапу с суммовыми вхождениями и складываем в слайс
	for doc, count := range wordsCount {
		// Проверяем, сколько вхождений у документа
		if count != 0 {
			// Добавляем в слайс только те документы, у которых количество вхождений не равно 0
			wordsCountSorted = append(wordsCountSorted, [2]int{doc, count})
		}
	}
	// Определимся с ограничением на вывод результатов
	// Берем лимит и длину слайса
	limit := s.limit
	lenRelevance := len(wordsCountSorted)
	// Проверяем, что короче
	if lenRelevance < limit {
		// Слайс короче лимита
		limit = lenRelevance
	}
	// Какое-то подобие обрезанной сортировки пузырьком
	// Выполняем столько прогонов, сколько результатов нам нужно вывести
	for i := 0; i < limit; i++ {
		// На каждую единицу результата обходим слайс количеств справа и передвигаем максимальные влево
		for j := lenRelevance-1; j > i; j-- {
			// Проверяем, надо ли поменять элементы местами, при помощи функции-компаратора
			if less(wordsCountSorted[j-1], wordsCountSorted[j]) == -1 {
				// Элемент слева меньше элемента справа, меняем местами
				wordsCountSorted[j-1], wordsCountSorted[j] = wordsCountSorted[j],wordsCountSorted[j-1]
			}
		}
	}
	// Инициируем слайс для релевантности документов
	relevance := make([]int, 0)
	// Обходим отсортированный слайс с релевантностями документов
	// Пока не уперлись в ограничитель или не закончился слайс релевантностей
	for i := 0; i < limit; i++ {
		relevance = append(relevance, wordsCountSorted[i][0])
	}
	// Возвращаем получившийся слайс
	return relevance
}

// NewSearchEngine - конструктор поисковой системы
func NewSearchEngine(limit int) *SearchEngine {
	return &SearchEngine{
		index: make(map[string]map[int]int), // Поисковый индекс
		limit: limit, // Ограничитель по количеству выводимых документов
	}
}

// getUniqueWords - возвращает уникальные слова переданной строки (запроса)
func getUniqueWords(query string) []string {
	// Инициируем мапу для уникальных слов
	wordsCount := make(map[string]int)
	// Полученный запрос-строку разбиваем по пробелам на слова и обходим в цикле
	for _, word := range strings.Split(query, " ") {
		// Вообще, счетчик нам не нужен, но пусть будет
		wordsCount[word]++
	}
	// Слайс строк для уникальных слов
	result := make([]string, 0)
	// Обходим мапу в цикле и складываем слова в слайс
	for word := range wordsCount {
		result = append(result, word)
	}
	// Возвращаем слайс слов
	return result
}

// less - сравнивает переданные массивы с номером документа и количеством вхождений
func less(a,b [2]int) int {
	// Сначала сравниваем количество вхождений
	// Больше вхождений - меньше позиция документа
	if res := cmp.Compare(a[1], b[1]); res != 0 {
		return res
	}
	// Затем сравниваем номера документов
	// Больше номер - больше позиция документа
	if res := cmp.Compare(b[0], a[0]); res != 0 {
		return res
	}
	// Всё совпало, позиция не меняется
	return 0
}

func main() {
	// Создаем поисковую систему с лимитом вывода документов = 5
	searchEngine := NewSearchEngine(5)
	// Создаем сканер
	scanner := makeScanner()
	// Считываем с ввода количество документов
	n := readInt(scanner)
	// Считываем строки-документы и обновляем поисковый индекс по каждой
	for i := 0; i < n; i++ {
		searchEngine.updateIndex(i, readLine(scanner))
	}
	// Считываем количество запросов
	m := readInt(scanner)
	// Инициируем слайс слайсов для вывода результатов
	result := make([][]int, m)
	// Считываем строки-запросы и обрабатываем каждую при помощи поисковой системы
	for i := 0; i < m; i++ {
		// Результат складываем в слайс
		result[i] = searchEngine.processQuery(readLine(scanner))
	}
	// Выводим результаты
	for i := 0; i < m; i++ {
		// Результат обработки каждого запроса - слайс
		printArray(result[i])
	}
}

func makeScanner() *bufio.Scanner {
	const maxCapacity = 10 * 1024 * 1024
	buf := make([]byte, maxCapacity)
	scanner := bufio.NewScanner(os.Stdin)
	scanner.Buffer(buf, maxCapacity)
	return scanner
}

func readInt(scanner *bufio.Scanner) int {
	scanner.Scan()
	stringInt := scanner.Text()
	res, _ := strconv.Atoi(stringInt)
	return res
}

func readLine(scanner *bufio.Scanner) string {
	scanner.Scan()
	return scanner.Text()
}

func printArray(arr []int) {
	writer := bufio.NewWriter(os.Stdout)
	for i := 0; i < len(arr); i++ {
		writer.WriteString(strconv.Itoa(arr[i]))
		writer.WriteString(" ")
	}
	writer.WriteString("\n")
	writer.Flush()
}